{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eae86f7",
   "metadata": {},
   "source": [
    "# 02 - ML Experimentation with Custom Model\n",
    "\n",
    "The purpose of this notebook is to use [custom training](https://cloud.google.com/ai-platform-unified/docs/training/custom-training) to train a TFDF classifier. The notebook covers the following tasks:\n",
    "1. Preprocess the data locally using Apache Beam.\n",
    "2. Train and test custom model locally using a TFDF implementation.\n",
    "3. Submit a Dataflow job to preprocess the data.\n",
    "4. Submit a custom training job to Vertex AI using a [pre-built container](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "5. Upload the trained model to Vertex AI.\n",
    "6. Track experiment parameters from [Vertex AI Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction).\n",
    "\n",
    "We use [Vertex TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) \n",
    "and [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) to track, visualize, and compare ML experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5205917",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c6cfe",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hp_tuning\n",
    "\n",
    "from src.common import features, datasource_utils\n",
    "from src.model_training import data, model_tfdf, defaults, trainer, exporter\n",
    "from src.preprocessing import etl\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")\n",
    "print(f\"TensorFlow Decision Forests: {tfdf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde361cf",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fce242",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = '' # Change to your project id.\n",
    "REGION = '' # Change to your region.\n",
    "BUCKET =  ''\n",
    "SERVICE_ACCOUNT = \"\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn't exist\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "PARENT = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)\n",
    "print(\"Vertex API Parent URI:\", PARENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfd33f",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'tfdf-rules'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "\n",
    "WORKSPACE = f'gs://{BUCKET}/{DATASET_DISPLAY_NAME}'\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "RAW_SCHEMA_LOCATION = 'src/raw_schema/schema.pbtxt'\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{DATASET_DISPLAY_NAME}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "LOCAL_CSV_DIR = os.path.join('.', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf63d9",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_resource = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "tensorboard_resource_name = tensorboard_resource.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a3859",
   "metadata": {},
   "source": [
    "## Initialize workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "print(\"Experiment directory:\", EXPERIMENT_ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08e503",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aefc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "run_id = 'run-local-20220421180000'\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)\n",
    "\n",
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f92936",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "vertex_ai.start_run(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8cee2",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data\n",
    "\n",
    "Note that it is not necessary to preprocess data when using Random Forests (https://www.tensorflow.org/decision_forests/migration#feature_normalization_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d252ab",
   "metadata": {},
   "source": [
    "## 2. Train a custom model locally using Decision Forests\n",
    "\n",
    "The `TFDF` implementation of the custom model is in the [model_training](src/model_training) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d451ef7",
   "metadata": {},
   "source": [
    "### Train the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_pattern = os.path.join(LOCAL_CSV_DIR, 'data.csv')\n",
    "eval_data_file_pattern = os.path.join(LOCAL_CSV_DIR, 'data.csv')\n",
    "\n",
    "print(f\"train_data_file_pattern: {train_data_file_pattern}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trainer.train(\n",
    "    fn_args=None,\n",
    "    csv_data_dir=train_data_file_pattern,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6812469",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bafbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = trained_model.make_inspector().training_logs()\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58537477",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"\"  # Replace with path to the tensorboard directory on your machine\n",
    "%load_ext tensorboard\n",
    "trained_model.make_inspector().export_to_tensorboard(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd266da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.make_inspector().variable_importances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73653cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdf.model_plotter.plot_model_in_colab(trained_model, tree_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = trained_model.make_inspector().extract_tree(tree_idx=0)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7265916",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = trainer.evaluate(trained_model, train_data_file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a20b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_resource_name = 'projects/692954754682/locations/us-central1/tensorboards/1730314642770624512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tb-gcp-uploader --tensorboard_resource_name={tensorboard_resource_name} \\\n",
    "  --logdir={LOG_DIR} \\\n",
    "  --experiment_name={EXPERIMENT_NAME} --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44268d04",
   "metadata": {},
   "source": [
    "### Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = os.path.join(EXPORT_DIR)\n",
    "\n",
    "print(f\"saved_model_dir: {saved_model_dir}\")\n",
    "print(f\"RAW_SCHEMA_LOCATION: {RAW_SCHEMA_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter.export_serving_model(\n",
    "    trained_model,\n",
    "    saved_model_dir,\n",
    "    RAW_SCHEMA_LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1ba3e",
   "metadata": {},
   "source": [
    "### Inspect model serving signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8989c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68ecd0",
   "metadata": {},
   "source": [
    "### Test the exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_model = tf.saved_model.load(saved_model_dir)\n",
    "print(\"Saved model is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f041d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the serving_default with feature dictionary\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"feature_1\": 1,\n",
    "    \"feature_2\": 0,\n",
    "    \"feature_3\": 1,\n",
    "    \"feature_4\": 1,\n",
    "    \"feature_5\": 0,\n",
    "}\n",
    "\n",
    "for feature_name in instance:\n",
    "    dtype = raw_feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = serving_model.signatures['serving_default'](**instance)\n",
    "for key in predictions:\n",
    "    print(f\"{key}: {predictions[key].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec2080",
   "metadata": {},
   "source": [
    "## Start a new Vertex AI experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME)\n",
    "\n",
    "run_id = f\"run-gcp-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4dfa8",
   "metadata": {},
   "source": [
    "## 4. Submit a Custom Training Job to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e998139",
   "metadata": {},
   "source": [
    "### Test the training task locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.model_training.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={train_data_file_pattern} \\\n",
    "    --eval-data-dir={eval_data_file_pattern}  \\\n",
    "    --experiment-name={EXPERIMENT_NAME} \\\n",
    "    --run-name={run_id} \\\n",
    "    --project={PROJECT} \\\n",
    "    --region={REGION} \\\n",
    "    --staging-bucket={BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077e9d4",
   "metadata": {},
   "source": [
    "### Prepare training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_PACKAGE_DIR = os.path.join(WORKSPACE, 'trainer_packages')\n",
    "TRAINER_PACKAGE_NAME = f'{MODEL_DISPLAY_NAME}_trainer'\n",
    "print(\"Trainer package upload location:\", TRAINER_PACKAGE_DIR)\n",
    "print(f\"TRAINER_PACKAGE_NAME: {TRAINER_PACKAGE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cc9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm -r src/__pycache__/\n",
    "#rm -r src/.ipynb_checkpoints/\n",
    "#rm -r src/raw_schema/.ipynb_checkpoints/\n",
    "!rm -f {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}.tar.gz\n",
    "\n",
    "!mkdir {TRAINER_PACKAGE_NAME}\n",
    "\n",
    "!cp setup.py {TRAINER_PACKAGE_NAME}/\n",
    "!cp -r src {TRAINER_PACKAGE_NAME}/\n",
    "!cp -r data {TRAINER_PACKAGE_NAME}/\n",
    "!tar cvf {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}\n",
    "!gzip {TRAINER_PACKAGE_NAME}.tar\n",
    "!gsutil cp {TRAINER_PACKAGE_NAME}.tar.gz {TRAINER_PACKAGE_DIR}/\n",
    "!rm -r {TRAINER_PACKAGE_NAME}\n",
    "!rm -r {TRAINER_PACKAGE_NAME}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3af757",
   "metadata": {},
   "source": [
    "### Prepare the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af6480",
   "metadata": {},
   "source": [
    "A custom job with a custom image can't be used. An error is thrown: \"Please use an image offered by Vertex AI for python package training.\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m79"
  },
  "interpreter": {
   "hash": "969d35e749fda7fadd47ba635ad15fe8638793cf4bed406010c22418a542c5ee"
  },
  "kernelspec": {
   "display_name": "clearsafety-rules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
